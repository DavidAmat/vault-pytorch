{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "3a395ce9-9284-4e3f-b7aa-d96c4a96b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from os.path import join as jp\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import matplotlib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1659681d-f1f2-4de2-a27c-10c8798c6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim, Tensor\n",
    "\n",
    "from torch_geometric.utils import structured_negative_sampling, structured_negative_sampling_feasible\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn import LGConv\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "5baa25ad-770e-4bf9-97ef-119dc350af6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "utils = reload(utils)\n",
    "get_metrics = utils.get_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a279dc43-a9e5-4435-ba54-880cbc4a1f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fe6609c5990>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c8eaa0-5169-4842-b09b-626f9262e882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9922a6-7094-44ce-bc51-f5fd0f3eac30",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "0dea910f-6a54-48ee-896e-a37324677f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bpr_loss(emb_users_final, emb_users, \n",
    "             emb_pos_items_final, emb_pos_items, \n",
    "             emb_neg_items_final, emb_neg_items\n",
    "):\n",
    "    # Tensors size\n",
    "    #    emb_users_final: [batch_size, embedding_dim]\n",
    "    \n",
    "    # Regularization term (norm of the ORIGINAL embedding, not the propagated one)\n",
    "    reg_loss = LAMBDA * (emb_users.norm().pow(2) +\n",
    "                        emb_pos_items.norm().pow(2) +\n",
    "                        emb_neg_items.norm().pow(2))\n",
    "    \n",
    "    # We want the dot product to be computed like:\n",
    "    # emb_users_final[0] @ emb_pos_items_final[0]\n",
    "    # emb_users_final[1] @ emb_pos_items_final[1]\n",
    "    # to achieve this, this is the same as doing pairwise multiplication (torch.mul)\n",
    "    # of the batched tensors and then apply a sumation over the rows\n",
    "    # so that we will end up with a pos_ratings of size [batch_size]\n",
    "    # which each index (0, 1) will have the dot product of user embedding vs. item embedding\n",
    "\n",
    "    # Dot product of the propagated user embedding with the propagated positive item embedding\n",
    "    pos_ratings = torch.mul(emb_users_final, emb_pos_items_final).sum(dim=-1)\n",
    "    \n",
    "    # Dot product of the propagated user embedding with the propagated negative item embedding\n",
    "    neg_ratings = torch.mul(emb_users_final, emb_neg_items_final).sum(dim=-1)\n",
    "\n",
    "    \n",
    "    # careful using this, because bpr_loss here is always positive in softplus\n",
    "    # hence, we always want the loss function to contribute positively\n",
    "    # bpr_loss = torch.mean(torch.nn.functional.softplus(pos_ratings - neg_ratings))\n",
    "    \n",
    "    # if we choose the paper implementation, we use LogSigmoid (always negative)\n",
    "    # this is why we will change the sign from negative to positive in the final loss computation\n",
    "    bpr_loss = torch.mean(torch.nn.functional.logsigmoid(pos_ratings - neg_ratings))\n",
    "    \n",
    "    # consider adding regularization loss (always positive since we are adding norms)\n",
    "    return -bpr_loss + reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac11e01-1023-4c80-98de-2b3c16b48125",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "3e1d29e9-c465-4c5d-ab60-cd908d3a0604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightGCN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_users, num_items, \n",
    "                 edge_index, edge_values,\n",
    "                 edge_index_val=None, edge_values_val=None,\n",
    "                 num_layers=4, dim_h=64, batch_size=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_users = nn.Embedding(num_embeddings=self.num_users, embedding_dim=dim_h)\n",
    "        self.emb_items = nn.Embedding(num_embeddings=self.num_items, embedding_dim=dim_h)\n",
    "        self.edge_index = edge_index\n",
    "        self.edge_values = edge_values\n",
    "        self.adj_mat = self.compute_norm_adj_matrix(edge_index, edge_values)\n",
    "        self.sp_adj_mat = self._convert_sp_mat_to_sp_tensor(self.adj_mat)\n",
    "        self.alpha = 1/(self.num_layers+1)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # self.convs = nn.ModuleList(LGConv() for _ in range(num_layers))\n",
    "\n",
    "        nn.init.normal_(self.emb_users.weight, std=0.01)\n",
    "        nn.init.normal_(self.emb_items.weight, std=0.01)\n",
    "        \n",
    "        # Construct positive and negative edges\n",
    "        self._generate_positives_negative_edges()\n",
    "        \n",
    "        # Validation adjacency matrix\n",
    "        self.edge_index_val = edge_index_val\n",
    "        self.edge_values_val = edge_values_val\n",
    "        self.val_adj_mat = self.compute_norm_adj_matrix(edge_index_val, edge_values_val, is_valid=True)        \n",
    "        self.sp_val_adj_mat = self._convert_sp_mat_to_sp_tensor(self.val_adj_mat)\n",
    "    \n",
    "    def _generate_positives_negative_edges(self):\n",
    "        edge_index = self.edge_index\n",
    "\n",
    "        # Generate negative sample indices\n",
    "        # IMPORTANT! let's consider only as num_nodes the size of the items set\n",
    "        # this is to avoid the case where num_users > num_items and we sample\n",
    "        # from a user_id that is higher than a item_id and we will be providing \n",
    "        # and index for the item_id that does not exist\n",
    "        edge_index = structured_negative_sampling(edge_index, num_nodes=self.num_items)\n",
    "        \n",
    "        # edge_index: Tuple of 3 tensors\n",
    "        # tensor1: indices of user node\n",
    "        # tensor2: indices of item node (positive interaction with user)\n",
    "        # tensor3: indices of the item node (negative interaction with user)\n",
    "        self.pos_neg_edges = torch.stack(edge_index, dim=0)       \n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def _convert_sp_mat_to_sp_tensor(X):\n",
    "        coo = X.tocoo().astype(np.float32)\n",
    "        row = torch.Tensor(coo.row).long()\n",
    "        col = torch.Tensor(coo.col).long()\n",
    "        index = torch.stack([row, col])\n",
    "        data = torch.FloatTensor(coo.data)\n",
    "        return torch.sparse.FloatTensor(index, data, torch.Size(coo.shape))\n",
    "    \n",
    "    def compute_norm_adj_matrix(self, edge_index, edge_values, is_valid=False):\n",
    "        num_users = self.num_users\n",
    "        num_items = self.num_items\n",
    "        # Interaction matrix\n",
    "        R = sp.coo_matrix((\n",
    "            edge_values, \n",
    "            (edge_index[0], edge_index[1])),\n",
    "            shape=(num_users, num_items))\n",
    "        R = R.tolil()\n",
    "\n",
    "        # Save interaction matrix\n",
    "        if not is_valid:\n",
    "            self.R = R\n",
    "        else:\n",
    "            self.R_valid = R\n",
    "        \n",
    "        # Adjacency matrix\n",
    "        MN = self.num_users + self.num_items\n",
    "        adj_mat = sp.dok_matrix((MN, MN), dtype=np.float32)\n",
    "        adj_mat = adj_mat.tolil()\n",
    "        \n",
    "        # Fill adjacency matrix\n",
    "        adj_mat[:num_users, num_users:] = R\n",
    "        adj_mat[num_users:, :num_users] = R.T\n",
    "        \n",
    "        # Degrees\n",
    "        rowsum = np.array(adj_mat.sum(1))\n",
    "        \n",
    "        # Inverse of the Degree matrix\n",
    "        d_inv = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv[np.isinf(d_inv)] = 0.\n",
    "        d_mat = sp.diags(d_inv)\n",
    "        \n",
    "        # Normalized Adjacency Matrix\n",
    "        norm_adj = d_mat.dot(adj_mat)\n",
    "        norm_adj = norm_adj.dot(d_mat)\n",
    "        return norm_adj\n",
    "    \n",
    "    def sample_mini_batch(self):\n",
    "        # Generate BATCH_SIZE random indices (size: [batch_size])\n",
    "        index = np.random.choice(range(self.edge_index.shape[1]), size=self.batch_size)\n",
    "        \n",
    "        # With such index, select the positive-negative edge pairs generated before\n",
    "        # first dimension is user, second is positive item, third is negative item\n",
    "        # sampled_pos_neg_edges: [3 x len(indices)]\n",
    "        sampled_pos_neg_edges = model.pos_neg_edges[:, index]\n",
    "        \n",
    "        # user_indices: [[1 x len(indices)]] (first row of pos_neg_edges sampled)\n",
    "        # etc...\n",
    "        user_indices, pos_item_indices, neg_item_indices = sampled_pos_neg_edges.numpy()\n",
    "\n",
    "        return user_indices, pos_item_indices, neg_item_indices\n",
    "        \n",
    "\n",
    "    def forward(self, is_valid=False):\n",
    "        # Keep track of starting embeddings for feeding into the BPR Loss \n",
    "        # for regularizing the learned embedding params\n",
    "        emb0_users = self.emb_users.weight\n",
    "        emb0_items = self.emb_items.weight\n",
    "                \n",
    "        # Embedding is dimension M + N\n",
    "        emb = torch.cat([emb0_users, emb0_items])\n",
    "        embs = [emb]\n",
    "\n",
    "        # For each layer\n",
    "        for layer_i in range(self.num_layers):\n",
    "            if not is_valid:\n",
    "                emb = torch.sparse.mm(self.sp_adj_mat, emb)\n",
    "            else:\n",
    "                emb = torch.sparse.mm(self.sp_val_adj_mat, emb)\n",
    "            embs.append(emb)\n",
    "\n",
    "        emb_final = self.alpha * torch.mean(torch.stack(embs, dim=1), dim=1)\n",
    "\n",
    "        embf_users, embf_items = torch.split(emb_final, [self.num_users, self.num_items])\n",
    "\n",
    "        return embf_users, emb0_users, embf_items, emb0_items\n",
    "    \n",
    "    # EVALUATION\n",
    "    \n",
    "    # Validation loss\n",
    "    def valid_loss(self):\n",
    "        # Forward pass using the validation adjacency matrix\n",
    "        emb_users_final, emb_users, emb_items_final, emb_items = self.forward(is_valid=True)\n",
    "\n",
    "        # Choose negative sampling\n",
    "        user_indices, pos_item_indices, neg_item_indices = structured_negative_sampling(\n",
    "            self.edge_index_val, \n",
    "            num_nodes=self.num_items,\n",
    "            contains_neg_self_loops=False\n",
    "        )\n",
    "\n",
    "        # Applying sample indices\n",
    "        s_embf_users, s_emb0_users = embf_users[user_indices], emb0_users[user_indices]\n",
    "        s_embf_items_pos, s_emb0_items_pos = embf_items[pos_item_indices], emb0_items[pos_item_indices]\n",
    "        s_embf_items_neg, s_emb0_items_neg = embf_items[neg_item_indices], emb0_items[neg_item_indices]\n",
    "\n",
    "\n",
    "        # Loss computation\n",
    "        valid_loss = bpr_loss(\n",
    "            s_embf_users, s_emb0_users, \n",
    "            s_embf_items_pos, s_emb0_items_pos, \n",
    "            s_embf_items_neg, s_emb0_items_neg\n",
    "        ).item()\n",
    "\n",
    "        #recall, ndcg = get_metrics(model, edge_index, exclude_edge_indices)\n",
    "\n",
    "        return valid_loss # , recall, ndcg\n",
    "    \n",
    "    def get_val_metrics(self, epoch: int, topk_recs=10, k_list=[1,2,3]):\n",
    "    \n",
    "        # Get ratings by embeddings dot products\n",
    "        ratings = torch.matmul(self.emb_users.weight, self.emb_items.weight.T)\n",
    "\n",
    "\n",
    "        # Exclude interactions in the train_set\n",
    "        excl_user_indices, excl_item_indices = self.edge_index\n",
    "        ratings[excl_user_indices, excl_item_indices] = -1024\n",
    "\n",
    "        # get the top k recommended items for each user\n",
    "        _, top_K_items = torch.topk(ratings, k=topk_recs)\n",
    "\n",
    "        # Get metrics\n",
    "        model.edge_index_val\n",
    "        l_metrics = get_metrics(\n",
    "            top_rec_items=top_K_items,\n",
    "            ground_truth=self.edge_index_val,\n",
    "            k_list=k_list\n",
    "        )\n",
    "        l_metrics = [(epoch,) + tup for tup in l_metrics]\n",
    "        # Convert to dataframe\n",
    "        #df_metrics_epoch = pd.DataFrame(l_metrics, columns=[\"epoch\", \"K\", \"TP\", \"FP\", \"P\", \"precision\", \"recall\"])\n",
    "\n",
    "        return l_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deeb7b82-0346-4a6e-9363-2110775e1925",
   "metadata": {},
   "source": [
    "### Instantiate the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "fc61e58e-8018-44d9-a18b-4dfa87f11d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_edge_index = torch.tensor(\n",
    "    [[0, 0, 1, 1, 2, 2, 3, 3, 4, 4],\n",
    "     [0, 1, 0, 2, 1, 2, 1, 2, 0, 2]]\n",
    ")\n",
    "\n",
    "toy_valid_edge_index = torch.tensor(\n",
    "    [[0, 1, 2],\n",
    "     [2, 1, 0]]\n",
    ")\n",
    "\n",
    "toy_edge_index = torch.LongTensor(toy_edge_index) \n",
    "toy_edge_values = torch.ones_like(toy_edge_index[0])\n",
    "toy_valid_edge_values = torch.ones_like(toy_valid_edge_index[0])\n",
    "\n",
    "nu = 5\n",
    "ni = 3\n",
    "embdi = 3\n",
    "layers = 1 \n",
    "\n",
    "model = LightGCN(\n",
    "    num_users=nu, \n",
    "    num_items=ni, \n",
    "    edge_index=toy_edge_index,\n",
    "    edge_values=toy_edge_values,\n",
    "    edge_index_val=toy_valid_edge_index,\n",
    "    edge_values_val=toy_valid_edge_values,\n",
    "    num_layers=1, \n",
    "    dim_h=embdi\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "c551d3df-ed62-4d17-b801-dab9e5cce92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward\n",
    "embf_users, emb0_user, embf_items, emb0_items = model.forward(is_valid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943588d7-01c2-4ab9-83d8-e12ee1897dd5",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "d13108f8-d3af-4d54-b985-d0d34464c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 1e-6\n",
    "K_LIST = [1,2]\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 30\n",
    "n_samples_interaction = toy_edge_index.shape[1]\n",
    "n_batches = n_samples_interaction // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "7b530eee-ed52-403e-a62f-053abdf33a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abdac3c6-448d-4c4e-9035-ce39c20d050f",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "6a48054f-cef1-424d-beb2-441c1d083949",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LightGCN(\n",
    "    num_users=nu, \n",
    "    num_items=ni, \n",
    "    edge_index=toy_edge_index,\n",
    "    edge_values=toy_edge_values,\n",
    "    edge_index_val=toy_valid_edge_index,\n",
    "    edge_values_val=toy_valid_edge_values,\n",
    "    num_layers=1,\n",
    "    dim_h=embdi\n",
    ")\n",
    "model = model.to(device)\n",
    "model.edge_index = model.edge_index.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "24b83ea6-c73f-40e9-b7b6-7ceb726bde44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 0 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 0 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 0 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 0 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 0 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 5 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 5 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 5 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 5 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 5 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 10 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 10 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 10 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 10 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 10 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 15 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 15 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 15 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 15 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 15 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 20 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 20 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 20 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 20 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 20 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 25 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 25 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 25 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 25 Precision@2 - 0.5 Recall@2 - 1.0\n",
      "Epoch - 25 Precision@2 - 0.5 Recall@2 - 1.0\n"
     ]
    }
   ],
   "source": [
    "# Metrics loss\n",
    "l_metrics = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    \n",
    "    # Let's do a forward pass for all nodes\n",
    "    # and sample nodes to see if when testing the embedding BPR losses\n",
    "    # between two positive edges nodes, their embeddings are closer \n",
    "    # than a negative pair sampled too\n",
    "    for _ in range(n_batches):\n",
    "        # Forward pass\n",
    "        embf_users, emb0_users, embf_items, emb0_items = model.forward()\n",
    "        \n",
    "        # Getting sample indices\n",
    "        user_indices, pos_item_indices, neg_item_indices = model.sample_mini_batch()\n",
    "        \n",
    "        # Applying sample indices\n",
    "        s_embf_users, s_emb0_users = embf_users[user_indices], emb0_users[user_indices]\n",
    "        s_embf_items_pos, s_emb0_items_pos = embf_items[pos_item_indices], emb0_items[pos_item_indices]\n",
    "        s_embf_items_neg, s_emb0_items_neg = embf_items[neg_item_indices], emb0_items[neg_item_indices]\n",
    "        \n",
    "        # Loss computation\n",
    "        train_loss = bpr_loss(\n",
    "            s_embf_users, s_emb0_users, \n",
    "            s_embf_items_pos, s_emb0_items_pos, \n",
    "            s_embf_items_neg, s_emb0_items_neg\n",
    "        )\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 5 == 0:\n",
    "\n",
    "        # Precision and recall on validation (generate all items recs)\n",
    "        l_epoch_metrics = model.get_val_metrics(\n",
    "            epoch=epoch, \n",
    "            topk_recs=model.num_items,\n",
    "            k_list=[1,2]\n",
    "        )\n",
    "        l_metrics.append(l_epoch_metrics)\n",
    "\n",
    "\n",
    "        # If we want to print Prec@K with K=2, we will select the second item of k_list\n",
    "        k_print = 2\n",
    "        idx_k = np.where(np.array(K_LIST)==k_print)[0][0]\n",
    "        prec = l_epoch_metrics[idx_k][-2]\n",
    "        rec = l_epoch_metrics[idx_k][-1]\n",
    "\n",
    "        print(f\"Epoch - {epoch}\", f\"Precision@{k_print} - {prec}\", f\"Recall@{k_print} - {rec}\")     \n",
    "\n",
    "# df = pd.DataFrame(np.vstack(np.array(l_metrics)), columns=[\"epoch\", \"K\", \"TP\", \"FP\", \"P\", \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "8a6335d9-fa83-4304-83c2-c3f1c22a6207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>K</th>\n",
       "      <th>TP</th>\n",
       "      <th>FP</th>\n",
       "      <th>P</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>15.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>20.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>25.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch    K   TP   FP    P  precision  recall\n",
       "0     0.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "1     0.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "2     0.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "3     0.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "4     0.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "5     0.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "6     0.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "7     0.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "8     0.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "9     0.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "10    5.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "11    5.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "12    5.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "13    5.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "14    5.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "15    5.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "16    5.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "17    5.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "18    5.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "19    5.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "20   10.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "21   10.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "22   10.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "23   10.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "24   10.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "25   10.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "26   10.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "27   10.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "28   10.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "29   10.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "30   15.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "31   15.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "32   15.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "33   15.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "34   15.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "35   15.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "36   15.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "37   15.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "38   15.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "39   15.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "40   20.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "41   20.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "42   20.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "43   20.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "44   20.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "45   20.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "46   20.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "47   20.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "48   20.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "49   20.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "50   25.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "51   25.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "52   25.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "53   25.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "54   25.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "55   25.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "56   25.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "57   25.0  2.0  3.0  3.0  3.0        0.5     1.0\n",
       "58   25.0  1.0  3.0  0.0  3.0        1.0     1.0\n",
       "59   25.0  2.0  3.0  3.0  3.0        0.5     1.0"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "769cd48e-f93e-4155-b94a-58db97ab4e05",
   "metadata": {},
   "source": [
    "# get metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "1459befc-1d45-49a2-8dab-3594cd9f90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_rec_items = torch.tensor([\n",
    "            [2, 0, 1],\n",
    "            [0, 1, 2],\n",
    "            [0, 1, 2],\n",
    "            [0, 1, 2],\n",
    "            [1, 0, 2]])\n",
    "\n",
    "\n",
    "ground_truth = torch.tensor(\n",
    "    [[0, 0, 1, 1, 2],\n",
    "     [2, 0, 1, 2, 0]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "id": "36572a29-03e9-4511-a491-f303e1ec9537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7956)"
      ]
     },
     "execution_count": 638,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_at_k(top_rec_items, ground_truth, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "id": "5841389f-e17d-4430-baf5-865aba3d365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [2.3, 2.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "8c12b782-71d6-4127-9be5-6672c78db29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [(aa,) for aa in a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "3a5d0dfe-5604-4678-9639-2dcbcddd9ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2.3,), (2.1,)]"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a04b08d-7d3e-4310-a2c4-d62f84412a3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vault_pytorch_lightgcn",
   "language": "python",
   "name": "vault_pytorch_lightgcn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
